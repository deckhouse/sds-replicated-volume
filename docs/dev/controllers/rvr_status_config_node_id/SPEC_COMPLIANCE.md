# Соответствие спецификации для `rvr-status-config-node-id-controller`

> **Примечание:** Этот контроллер соответствует стандартам проекта, описанным в [`CONTROLLER_STYLE_GUIDE.md`](../CONTROLLER_STYLE_GUIDE.md).

## Спецификация (из `docs/dev/spec_v1alpha3.md`)

### Цель
Проставить свойству `rvr.status.config.nodeId` уникальное значение среди всех реплик одной RV, в диапазоне [0; 7].

В случае превышения количества реплик, повторять реконсайл с ошибкой.

### Триггер
- `CREATE(RVR, status.config.nodeId==nil)`

### Вывод
- `rvr.status.config.nodeId`

---

## Схема логики работы контроллера

```
┌─────────────────────────────────────────────────────────────────┐
│                    Триггеры контроллера                         │
├─────────────────────────────────────────────────────────────────┤
│ CREATE(RVR, status.config.nodeId==nil)                          │
└─────────────────────────────────────────────────────────────────┘
                            │
                            ▼
┌─────────────────────────────────────────────────────────────────┐
│                      Reconcile()                                │
├─────────────────────────────────────────────────────────────────┤
│ 1. Получить RVR по req.NamespacedName                           │
│ 2. Проверить: nodeId уже установлен?                            │
└─────────────────────────────────────────────────────────────────┘
                            │
                ┌───────────┴───────────┐
                │                       │
                ▼                       ▼
    ┌───────────────────┐   ┌──────────────────────────┐
    │ nodeId установлен │   │ nodeId не установлен     │
    └───────────────────┘   └──────────────────────────┘
                │                       │
                ▼                       ▼
    ┌────────────────────┐   ┌──────────────────────────────┐
    │ return (idempotent)│   │ Получить все RVR для того же │
    └────────────────────┘   │ ReplicatedVolume             │
                             └──────────────────────────────┘
                                        │
                                        ▼
                            ┌──────────────────────────────┐
                            │ Собрать занятые nodeIDs (0-7)│
                            └──────────────────────────────┘
                                        │
                        ┌───────────────┴───────────────┐
                        │                               │
                        ▼                               ▼
            ┌──────────────────────┐      ┌────────────────────┐
            │ totalReplicas > 8?   │      │ totalReplicas <= 8 │
            └──────────────────────┘      └────────────────────┘
                        │                               │
                        ▼                               ▼
            ┌──────────────────────┐      ┌──────────────────────────┐
            │ setErrorCondition()  │      │ Найти доступный nodeID   │
            │ return ErrInvalid    │      │ (0-7, заполняя пропуски) │
            └──────────────────────┘      └──────────────────────────┘
                                                    │
                                    ┌───────────────┴───────────────┐
                                    │                               │
                                    ▼                               ▼
                        ┌───────────────────────┐      ┌───────────────────────┐
                        │ availableNodeID == nil│      │ availableNodeID найден│
                        └───────────────────────┘      └───────────────────────┘
                                │                               │
                                ▼                               ▼
                ┌──────────────────────┐        ┌──────────────────────────┐
                │ setErrorCondition()  │        │ PatchStatusWithConflict  │
                │ return ErrInvalid    │        │ Retry()                  │
                └──────────────────────┘        │ 1. Проверить nodeId снова│
                                                │    (idempotent check)    │
                                                │ 2. Инициализировать      │
                                                │    status если нужно     │
                                                │ 3. Установить nodeID     │
                                                └──────────────────────────┘
                                                            │
                                                            ▼
                                                ┌──────────────────────────┐
                                                │ return (success)         │
                                                └──────────────────────────┘
```

### Описание схемы

**Триггеры:**
- `CREATE(RVR)` без `nodeId` → запускает присвоение уникального `nodeId`

**Основной поток:**
1. **Идемпотентная проверка:** Если `nodeId` уже установлен, выходим без изменений
2. **Сбор информации:** Получаем все RVR для того же `ReplicatedVolume` и собираем занятые `nodeId` (0-7)
3. **Проверка лимита:** Если реплик больше 8, возвращаем ошибку (максимум 8 реплик: nodeId 0-7)
4. **Поиск доступного nodeId:** Ищем первый свободный `nodeId` в диапазоне [0; 7], заполняя пропуски
5. **Проверка доступности:** Если все `nodeId` заняты, возвращаем ошибку
6. **Установка nodeId:** Используем `PatchStatusWithConflictRetry` для безопасной установки с повторной проверкой внутри `patchFn`

**Важно: Логика присвоения nodeId:**
- **Уникальность:** `nodeId` должен быть уникальным среди всех реплик одной `ReplicatedVolume`
- **Диапазон:** `nodeId` в диапазоне [0; 7] (всего 8 возможных значений)
- **Заполнение пропусков:** Если есть пропуски (например, заняты 0, 2, 3), новый `nodeId` будет 1 (первый свободный)
- **Параллелизм:** Используется `PatchStatusWithConflictRetry` для безопасной обработки параллельных запросов - если два воркера одновременно обрабатывают разные RVR, они получат разные `nodeId`

**Особенности:**
- Идемпотентность: проверки на каждом этапе (в начале `Reconcile` и внутри `patchFn`)
- Безопасность параллелизма: использование `PatchStatusWithConflictRetry` с optimistic locking
- Наблюдаемость: установка условий ошибки в status для диагностики

---

## Код, соответствующий спецификации

### ✅ `controller.go` - Триггеры

**Соответствует спецификации:**
- Использует `.For(&v1alpha3.ReplicatedVolumeReplica{})` для указания основного ресурса
- `CreateFunc` в `predicate.Funcs` (строки 29-35): Обрабатывает `CREATE(RVR, status.config.nodeId==nil)`
  - Проверяет, что `nodeId` не установлен
  - Возвращает `true` для обработки события

**Добавлено сверх спецификации (стандартная практика controller-runtime):**
- `GenericFunc` в `predicate.Funcs` (строки 45-51): Обрабатывает синхронизацию при старте контроллера
  - Это стандартная практика для reconciliation на старте
  - Не указано в спецификации, но необходимо для корректной работы
- Использование `builder.ControllerManagedBy` и `.For()` - соответствует стандартам проекта (см. `CONTROLLER_STYLE_GUIDE.md`)

**Не требуется спецификацией:**
- `UpdateFunc` (строки 37-40): No-op, так как `nodeId` неизменяем после установки
- `DeleteFunc` (строки 41-44): No-op, так как удаление не требует присвоения `nodeId`

### ✅ `reconciler.go` - Основная логика

**Соответствует спецификации:**

1. **Использование стандартного `reconcile.Reconciler`** (строки 40, 47-50)
   - Использует стандартный `reconcile.Request` вместо кастомных типов
   - Соответствует стандартам проекта (см. `CONTROLLER_STYLE_GUIDE.md`)

2. **Structured logging** (строка 51)
   - Использует `logr.Logger` с `.WithName()` и `.WithValues()`
   - Соответствует стандартам проекта

3. **Присвоение уникального `nodeId` в диапазоне [0; 7]** (строки 121-128)
   ```go
   // Find available nodeID
   var availableNodeID *uint
   for i := uint(minNodeID); i <= uint(maxNodeID); i++ {
       if !usedNodeIDs[i] {
           availableNodeID = &i
           break
       }
   }
   ```

4. **Проверка превышения количества реплик** (строки 96-118)
   ```go
   if totalReplicas > maxNodeID+1 {
       return reconcile.Result{}, e.ErrInvalidClusterf(...)
   }
   ```
   - Возвращает ошибку `ErrInvalidCluster`, что приводит к повторному reconcile

5. **Установка `rvr.status.config.nodeId`** (строки 150-179)
   - Использует `PatchStatusWithConflictRetry` для обновления статуса
   - Устанавливает `currentRVR.Status.Config.NodeId = availableNodeID`

6. **Сбор занятых `nodeId` среди всех реплик одной RV** (строки 70-94)
   - Получает все RVR для того же `ReplicatedVolumeName`
   - Собирает занятые `nodeId` в диапазоне [0; 7]

7. **Проверка, что `nodeId` еще не установлен** (строки 64-68)
   - Идемпотентность: если уже установлен, выходит без изменений

---

## Код, добавленный сверх спецификации

### ⚠️ `setNodeIDErrorCondition` - Улучшение наблюдаемости

**Файл:** `reconciler.go`, строки 204-238

**Статус:** НЕ в спецификации, добавлено для улучшения наблюдаемости

**Что делает:**
- Устанавливает `ConfigurationAdjusted=False` с `reason=ConfigurationFailed` в RVR status conditions
- Вызывается при ошибках:
  - Слишком много реплик (строка 104)
  - Все `nodeId` заняты (строка 136)
  - Все `nodeId` заняты при retry (строка 182)

**Почему добавлено:**
- Спецификация требует только возвращать ошибку
- Добавлено, чтобы администраторы видели проблему в RVR status conditions, а не только в логах контроллера

**Как откатить:**
- Удалить все вызовы `r.setNodeIDErrorCondition(...)` (строки 104, 136, 182)
- Удалить функцию `setNodeIDErrorCondition` (строки 204-238)
- Оставить только `return reconcile.Result{}, e.ErrInvalidClusterf(...)`

**Комментарии в коде:**
- Строки 99-103: "NOTE: Setting status condition is NOT in the spec..."
- Строки 131-135: "NOTE: Setting status condition is NOT in the spec..."
- Строки 207-213: "NOTE: This function and its usage are NOT in the spec..."

### ⚠️ `PatchStatusWithConflictRetry` - Обработка параллелизма

**Файл:** `reconciler.go`, строки 150-179

**Статус:** Техническая деталь реализации, не указана в спецификации

**Что делает:**
- Использует optimistic locking для безопасной параллельной обработки
- При конфликте (409) перезагружает ресурс и повторяет попытку
- Проверяет внутри `patchFn`, не установлен ли уже `nodeId` (idempotent check)

**Почему добавлено:**
- Спецификация не описывает технические детали обработки параллелизма
- Необходимо для корректной работы при параллельной обработке запросов несколькими воркерами
- Без этого возможны race conditions, когда два воркера присваивают один и тот же `nodeId`

**Соответствие спецификации:**
- ✅ Спецификация требует присвоение уникального `nodeId` - это обеспечивается
- ✅ Спецификация не запрещает использование retry механизмов
- Это техническая деталь реализации, необходимая для корректной работы

### ⚠️ `GenericFunc` - Reconciliation на старте

**Файл:** `controller.go`, строки 45-51

**Статус:** Стандартная практика controller-runtime, не указана в спецификации

**Что делает:**
- Обрабатывает события синхронизации при старте контроллера
- Проверяет все существующие RVR и добавляет в очередь те, у которых `nodeId` не установлен

**Почему добавлено:**
- Стандартная практика для Kubernetes контроллеров
- Обеспечивает обработку RVR, созданных до старта контроллера
- Не указано в спецификации, но необходимо для корректной работы

---

## Итоговая таблица соответствия

| Требование спецификации | Статус | Расположение в коде |
|------------------------|--------|-------------------|
| Присвоение уникального `nodeId` в диапазоне [0; 7] | ✅ Соответствует | `reconciler.go:121-128` |
| Проверка превышения количества реплик | ✅ Соответствует | `reconciler.go:96-118` |
| Возврат ошибки при превышении | ✅ Соответствует | `reconciler.go:113-118` |
| Триггер `CREATE(RVR, status.config.nodeId==nil)` | ✅ Соответствует | `controller.go:29-35` |
| Вывод `rvr.status.config.nodeId` | ✅ Соответствует | `reconciler.go:171` |
| Установка условия ошибки в status | ⚠️ Сверх спецификации | `reconciler.go:204-238` |
| Обработка параллелизма через retry | ⚠️ Техническая деталь | `reconciler.go:150-179` |
| Reconciliation на старте | ⚠️ Стандартная практика | `controller.go:45-51` |
| Стандартный reconcile.Reconciler | ✅ Соответствует стандартам | `reconciler.go:40, 47-50` |
| Использование .For() | ✅ Соответствует стандартам | `controller.go:27` |
| Structured logging | ✅ Соответствует стандартам | `reconciler.go:51` |
| Тестирование на Ginkgo/Gomega | ✅ Соответствует стандартам | `reconciler_test.go` |

---

## Итоги

1. **Код, соответствующий спецификации:** Оставлен как есть, полностью соответствует требованиям.

2. **Код сверх спецификации:**
   - `setNodeIDErrorCondition`: Можно оставить для улучшения наблюдаемости, или удалить для строгого соответствия спецификации
   - `PatchStatusWithConflictRetry`: Необходимо оставить для корректной работы при параллелизме
   - `GenericFunc`: Необходимо оставить для стандартной работы контроллера

3. **Тестирование:**
   - Используется **Ginkgo/Gomega** для структурированных тестов (соответствует `CONTROLLER_STYLE_GUIDE.md`)
   - Все тесты переписаны с использованием `Describe`/`It`/`BeforeEach` паттерна
   - Используется `GinkgoLogr` для интеграции логирования с Ginkgo
   - Покрытие: 9 тестов, включая happy path, edge cases, идемпотентность


