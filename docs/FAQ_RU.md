---
title: "Модуль SDS-DRBD: FAQ"
description: Диагностика проблем LINSTOR. Когда следует использовать LVM, а когда LVMThin? Производительность и надежность LINSTOR, сравнение с Ceph. Как добавить существующий LVM или LVMThin-пул в LINSTOR? Как настроить Prometheus на использование хранилища LINSTOR? Вопросы по работе контроллера.
---

{{< alert level="warning" >}}
Работоспособность модуля гарантируется только в следующих случаях:
- при использовании стоковых ядер, поставляемых вместе с [поддерживаемыми дистрибутивами](https://deckhouse.ru/documentation/v1/supported_versions.html#linux);
- при использовании сети 10Gbps.

Работоспособность модуля в других условиях возможна, но не гарантируется.
{{< /alert >}}

## Когда следует использовать LVM, а когда LVMThin?

Если кратко:

- LVM проще и обладает производительностью, сравнимой с производительностью накопителя;
- LVMThin позволяет использовать snapshot'ы и overprovisioning, но медленнее в два раза.

## Производительность и надежность LINSTOR, сравнение с Ceph

{% alert %}
Возможно, вам будет интересна наша статья [«Исследование производительности свободных хранилищ LINSTOR, Ceph, Mayastor и Vitastor в Kubernetes»](https://habr.com/ru/company/flant/blog/664150/).
{% endalert %}

Мы придерживаемся практического взгляда на вопрос. Разница в несколько десятков процентов на практике никогда не имеет значения. Имеет значение разница в несколько раз и более.

Факторы сравнения:

- Последовательные чтение и запись не имеют никакого значения, потому что на любой технологии они всегда упираются в сеть (что 10 Гбит/с, что 1 Гбит/с). С практической точки зрения этот показатель можно полностью игнорировать.
- Случайное чтение и запись (что на 1Гбит/с, что на 10Гбит/с):
    - DRBD + LVM в 5 раз лучше чем Ceph RBD (latency — в 5 раз меньше, IOPS — в 5 раз больше);
    - DRBD + LVM в 2 раза лучше чем DRBD + LVMThin.
- Если одна из реплик расположена локально, скорость чтения будет примерно равна скорости устройства хранения.
- Если нет реплик, расположенных локально, скорость записи будет примерно ограничена половиной пропускной способности сети при двух репликах или ⅓ пропускной способности сети при трех репликах.
- При большом количестве клиентов (больше 10, при iodepth 64) Ceph начинает отставать сильнее (до 10 раз) и потреблять значительно больше CPU.

В сухом остатке получается, что на практике не важно, какие параметры менять, и есть всего три значимых фактора:

- **Локальность чтения** — если все чтение производится локально, оно работает со скоростью (throughput, IOPS, latency) локального диска (разница практически незаметна).
- **1 сетевой hop при записи** — в DRBD репликацией занимается *клиент*, а в Ceph — *сервер*, поэтому у Ceph latency на запись всегда минимум в два раза больше, чем у DRBD.
- **Сложность кода** — latency вычислений на datapath (сколько процессорных команд выполняется на каждую операцию ввода/вывода), DRBD + LVM проще, чем DRBD + LVMThin, и значительно проще, чем Ceph RBD.

## Что использовать в какой ситуации?

По умолчанию модуль использует две реплики (третья — так называемая `diskless`, используется для поддержания кворума и создается автоматически). Такой подход гарантирует защиту от split-brain и достаточный уровень надежности хранения, но нужно учитывать следующие особенности:

- В момент недоступности одной из реплик (реплика A) данные записываются только в единственную реплику (реплика B). Это означает, что:
    - если в этот момент отключится и вторая реплика (реплика B), запись и чтение будут недоступны;
    - если при этом вторая реплика (реплика B) утеряна безвозвратно, данные будут частично потеряны (есть только старая реплика A);
    - если старая реплика (реплика A) была тоже утеряна безвозвратно, данные будут потеряны полностью.
- Чтобы включиться обратно при отключении второй реплики (без вмешательства оператора), требуется доступность обеих реплик. Это необходимо, чтобы корректно отработать ситуацию split-brain.
- Включение третьей реплики решает обе проблемы (в любой момент времени доступны минимум две копии данных), но увеличивает накладные расходы (сеть, диск).

Настоятельно рекомендуется иметь одну реплику локально. Это в два раза увеличивает возможную скорость записи (при двух репликах) и значительно увеличивает скорость чтения. Но даже если реплики на локальном хранилище нет, все также будет работать нормально, за исключением того, что чтение будет осуществляться по сети и будет двойная утилизация сети при записи.

В зависимости от задачи нужно выбрать один из следующих вариантов:

- DRBD + LVM — быстрее (в два раза) и надежнее (LVM — проще);
- DRBD + LVMThin — поддержка snapshot'ов и возможность overprovisioning.


## Как получить информацию об используемом пространстве?

Есть два варианта:

- Через дашборд Grafana: перейдите **Dashboards --> Storage --> LINSTOR/DRBD**  
  В правом верхнем углу вы найдете информацию об используемом пространстве в кластере.

  > **Внимание!** Эта информация отражает состояние всего *сырого* пространства в кластере.
  > То есть если вы создаете тома в двух репликах, то эти значения стоит поделить на два. Это нужно, чтобы получить примерное представление о том, сколько таких томов может быть размещено в вашем кластере.

- Через командный интерфейс LINSTOR:

  ```shell
  kubectl exec -n d8-sds-drbd deploy/linstor-controller -- linstor storage-pool list
  ```

  > **Внимание!** Эта информация отражает состояние *сырого* пространства для каждого узла в кластере.
  > То есть если вы создаете тома в двух репликах, то эти две реплики обязательно должны целиком поместиться на двух узлах вашего кластера.

## Как назначить StorageClass по умолчанию?
В соответствующем пользовательском ресурсе [DRBDStorageClass](ссылка на ресурс) в поле `spec.IsDefault` указать `true`.  

## Как добавить существующий LVM или LVMThin-pool?
Создать новый пользовательский ресурс [DRBDStoragePool](ссылка на ресурс), указав в поле `spec.Lvmvolumegroups.Name` соответствующий ресурс [LVMVolumeGroup](ссылка на ресурс) и при необходимости в поле `spec.Lvmvolumegroups.Thinpoolname` имя `LVMThin-pool`.

## Как настроить Prometheus на использование хранилища LINSTOR?

Чтобы настроить Prometheus на использование хранилища LINSTOR, необходимо:

- [настроить](configuration.html#конфигурация-хранилища-linstor) пулы хранения и StorageClass.
- указать параметры [longtermStorageClass](../300-prometheus/configuration.html#parameters-longtermstorageclass) и [storageClass](../300-prometheus/configuration.html#parameters-storageclass) в конфигурации модуля [prometheus](../300-prometheus/).

  Пример:

  ```yaml
  apiVersion: deckhouse.io/v1alpha1
  kind: ModuleConfig
  metadata:
    name: prometheus
  spec:
    version: 2
    enabled: true
    settings:
      longtermStorageClass: linstor-data-r2
      storageClass: linstor-data-r2
  ```

- дождаться перезапуска подов Prometheus.

## Как выгнать ресурсы с узла?

* Загрузите скрипт `evict.sh` на хост, имеющий доступ к API Kubernetes с правами администратора (для работы скрипта потребуются установленные `kubectl` и `jq`):

  * Последнюю версию скрипта можно скачать с GitHub:

    ```shell
    curl -fsSL -o evict.sh https://raw.githubusercontent.com/deckhouse/deckhouse/main/modules/041-linstor/tools/evict.sh
    chmod 700 evict.sh
    ```

  * Также скрипт можно скачать из пода `deckhouse`:

    ```shell
    kubectl -n d8-system cp -c deckhouse $(kubectl -n d8-system get po -l app=deckhouse -o jsonpath='{.items[0].metadata.name}'):/deckhouse/modules/041-linstor/tools/evict.sh ./evict.sh
    chmod 700 evict.sh
    ```

* Исправьте все ошибочные ресурсы LINSTOR в кластере. Чтобы найти их, выполните следующую команду:

  ```shell
  kubectl -n d8-sds-drbd exec -ti deploy/linstor-controller -- linstor resource list --faulty
  ```

* Убедитесь, что все поды в пространстве имен `d8-sds-drbd` находятся в состоянии *Running*:

  ```shell
  kubectl -n d8-sds-drbd get pods | grep -v Running
  ```

### Выгнать ресурсы с узла без удаления его из LINSTOR и Kubernetes

Запустите скрипт `evict.sh` в интерактивном режиме, указав режим удаления `--delete-resources-only`:

```shell
./evict.sh --delete-resources-only
```

Для запуска скрипта `evict.sh` в неинтерактивном режиме необходимо добавить флаг `--non-interactive` при его вызове, а также имя узла, с которого необходимо выгнать ресурсы. В этом режиме скрипт выполнит все действия без запроса подтверждения от пользователя. Пример вызова:

```shell
./evict.sh --non-interactive --delete-resources-only --node-name "worker-1"
```

> **Важно!** После завершении работы скрипта узел в Kubernetes останется в статусе *SchedulingDisabled*, а в LINSTOR у данного узла будет выставлен параметр *AutoplaceTarget=false*, что запретит планировщику LINSTOR создавать на этом узле ресурсы.

Если необходимо снова разрешить размещать ресурсы и поды на узле, нужно выполнить команды:

```shell
alias linstor='kubectl -n d8-sds-drbd exec -ti deploy/linstor-controller -- linstor'
linstor node set-property "worker-1" AutoplaceTarget
kubectl uncordon "worker-1"
```

Проверить параметр *AutoplaceTarget* у всех узлов можно так (поле AutoplaceTarget будет пустым у тех узлов, на которых разрешено размещать ресурсы LINSTOR):

```shell
alias linstor='kubectl -n d8-sds-drbd exec -ti deploy/linstor-controller -- linstor'
linstor node list -s AutoplaceTarget
```

### Выгнать ресурсы с узла с последующим его удалением из LINSTOR и Kubernetes

Запустите скрипт `evict.sh` в интерактивном режиме, указав режим удаления `--delete-node`:

```shell
./evict.sh --delete-node
```

Для запуска скрипта `evict.sh` в неинтерактивном режиме необходимо добавить флаг `--non-interactive` при его вызове, а также имя узла, который необходимо удалить. В этом режиме скрипт выполнит все действия без запроса подтверждения от пользователя. Пример вызова:

```shell
./evict.sh --non-interactive --delete-node --node-name "worker-1"
```

  > **Важно!** В процессе выполнения скрипт удалит узел как из Kubernetes, так и из LINSTOR.

В этом режиме ресурсы физически с узла не удаляются. Для зачистки узла необходимо зайти на него и выполнить следующие действия:

  > **Внимание!** Выполнение этих действий приведет к уничтожению всех ваших данных на узле.

* Получите список групп томов (vg), которые использовались для LVM-пулов хранения LINSTOR, а затем удалите их с узла:

  ```shell
  vgs -o+tags | awk 'NR==1;$NF~/linstor-/'
  vgremove -y <имена групп томов (vg) из вывода предыдущей команды>
  ```

* Получите список логических томов (lv), которые использовались для LVM_THIN-пулов хранения LINSTOR, а затем удалите их с узла:

  ```shell
  lvs -o+tags | awk 'NR==1;$NF~/linstor-/'
  lvremove -y /dev/<имя группы томов (vg) из вывода предыдущей команды>/<имя логического тома (lv) из вывода предыдущей команды>
  ```

* Следуйте [инструкции](../040-node-manager/faq.html#как-зачистить-узел-для-последующего-ввода-в-кластер), начиная со второго пункта, для дальнейшей очистки узла.

## Диагностика проблем

Проблемы могут возникнуть на разных уровнях работы компонентов.
Эта простая шпаргалка поможет вам быстро сориентироваться при диагностике различных проблем с томами, созданными в LINSTOR:

![LINSTOR шпаргалка](../../images/041-linstor/linstor-debug-cheatsheet.svg) <!--- исправить ссылку на изображение (возможно перенести его в модуль) --->
<!--- Исходник: https://docs.google.com/drawings/d/19hn3nRj6jx4N_haJE0OydbGKgd-m8AUSr0IqfHfT6YA/edit --->

Некоторые типичные проблемы описаны ниже.

### linstor-node не может запуститься из-за невозможности загрузки drbd-модуля

Проверьте состояние подов `linstor-node`:

```shell
kubectl get pod -n d8-sds-drbd -l app=linstor-node
```

Если вы видите, что некоторые из них находятся в состоянии `Init`, проверьте версию drbd и логи bashible на узле:

```shell
cat /proc/drbd
journalctl -fu bashible
```

Наиболее вероятные причины, почему он не может загрузить модуль ядра:

- Возможно, у вас уже загружена in-tree-версия модуля DRBDv8, тогда как LINSTOR требует DRBDv9.
  Проверить версию загруженного модуля: `cat /proc/drbd`. Если файл отсутствует, значит, модуль не загружен и проблема не в этом.

- Возможно, у вас включен Secure Boot.
  Так как модуль DRBD, который мы поставляем, компилируется динамически для вашего ядра (аналог dkms), он не имеет цифровой подписи.
  На данный момент мы не поддерживаем работу модуля DRBD в конфигурации с Secure Boot.

### Под не может запуститься из-за ошибки `FailedMount`

#### **Под завис на стадии `ContainerCreating`**

Если под завис на стадии `ContainerCreating`, а в выводе `kubectl describe pod` есть ошибки вида:

```text
rpc error: code = Internal desc = NodePublishVolume failed for pvc-b3e51b8a-9733-4d9a-bf34-84e0fee3168d: checking
for exclusive open failed: wrong medium type, check device health
```

значит, устройство все еще смонтировано на одном из других узлов.

Проверить это можно с помощью следующей команды:

```shell
linstor resource list -r pvc-b3e51b8a-9733-4d9a-bf34-84e0fee3168d
```

Флаг `InUse` укажет, на каком узле используется устройство.

#### **Под не может запуститься из-за отсутствия CSI-драйвера**

Пример ошибки в `kubectl describe pod`:

```text
kubernetes.io/csi: attachment for pvc-be5f1991-e0f8-49e1-80c5-ad1174d10023 failed: CSINode b-node0 does not
contain driver linstor.csi.linbit.com
```

Проверьте состояние подов `linstor-csi-node`:

```shell
kubectl get pod -n d8-sds-drbd -l app.kubernetes.io/component=csi-node
```

Наиболее вероятно, что они зависли в состоянии `Init`, ожидая, пока узел сменит статус на `Online` в LINSTOR. Проверьте список узлов с помощью следующей команды:

```shell
linstor node list
```

Если вы видите какие-либо узлы в состоянии `EVICTED`, значит, они были недоступны в течение 2 часов. Чтобы вернуть их в кластер, выполните:

```shell
linstor node rst <name>
```

#### **Ошибки вида `Input/output error`**

Такие ошибки обычно возникают на стадии создания файловой системы (mkfs).

Проверьте `dmesg` на узле, где запускается под:

```shell
dmesg | grep 'Remote failed to finish a request within'
```

Если вывод команды не пустой (в выводе `dmesg` есть строки вида *"Remote failed to finish a request within ..."*), скорее всего, ваша дисковая подсистема слишком медленная для нормального функционирования DRBD.

## Создает ли модуль SDS-DRBD свои пользовательские ресурсы автоматически по существующим Storage Pools в LINSTOR или StorageClass в Kubernetes? 
Нет, модуль `SDS-DRBD` не создает и не удаляет `DRBDStoragePool` и `DRBDStorageClass` автоматически. Их может создать только пользователь, а контроллер лишь отображает в поле `Status` текущее состояние соответствующей операции.

## В поле spec ресурса DRBDStorageClass присутствуют «зоны». Что это такое? 
Зоны (zones) — это пользовательская абстракция, определяющая, на каких узлах будут создаваться тома с данными и их реплики. Принадлежность узла к той или иной зоне отображается в `Labels` узла по ключу `topology.kubernetes.io/zone`. 

## Я удалил ресурс DRBDStoragePool, но соответствующий Storage Pool в LINSTOR остался. Так и должно быть?
Да, в настоящий момент модуль `SDS-DRBD` не обрабатывает операции при удалении ресурса `DRBDStoragePool`.

## Я не могу обновить поля в spec у ресурса DRBDStorageClass. Это ожидаемое поведение? 
Да, поведение ожидаемое. В `spec` можно изменять только поле `isDefault`. Остальные поля в `spec` ресурса сделаны неизменяемыми.

## Не могу удалить ресурс DRBDStorageClass, как и связанный с ним StorageClass в Kubernetes. Что делать? 
Пожалуйста, проверьте поле `Status.Phase` удаляемого ресурса. `sds-drbd-controller` проведет необходимые операции удаления только в случае, если значение вышеупомянутого поля находится в состоянии `Created`.

> Причина иного состояния должна быть отображена в поле `Status.Reason`.

## Нужно ли мне самостоятельно очищать невалидные Storage Pool в LINSTOR в случае неудачного завершения операции создания через ресурс DRDBStoragePool?
Нет, наш модуль позаботится об этом. Мы знаем, что `LINSTOR` может создавать даже невалидные `Storage Pool`, и поэтому в случае неудачной операции по созданию контроллер автоматически удалит некорректно созданный `Storage Pool` в `LINSTOR`.

> Если ресурс не прошел стадию валидации со стороны контроллера, непосредственной операции создания `Storage Pool` не произойдет.

## Я обратил внимание, что при создании Storage Pool / StorageClass в соответствующем ресурсе была отображена ошибка, а после все исправилось и нужная мне сущность создалась. Это ожидаемое поведение?
Да, это поведение ожидаемо. Модуль автоматически повторит выполнение неудачной операции, если причиной ошибки послужили независящие от модуля обстоятельства (например, «моргнул» куб-апи).

## Я не нашел ответа на свой вопрос и испытываю проблемы с работой модуля. Что делать? 
Информация о причинах неудавшейся операции должна быть отображена в поле `Status.Reason` ресурсов `DRBDStoragePool` и `DRBDStorageClass`. 
Если предоставленной информации не хватает для идентификации проблемы, вы можете обратиться к логам контроллера.

## Миграция на DRBDStorageClass

StorageClass'ы в данном модуле управляются через ресурс DRBDStorageClass. Вручную StorageClass'ы создаваться не должны.

При миграции с модуля Linstor необходимо удалить старые StorageClass'ы и создать новые через ресурс DRBDStorageClass в соответствии с таблицей.

Обратите внимание, что в старых StorageClass нужно смотреть опцию из секции parameter самого StorageClass, а указывать соответствующую опцию при создании нового необходимо в DRBDStorageClass.

| параметр StorageClass                     | DRBDStorageClass      | Параметр по умолчанию | Примечания                                                     |
|-------------------------------------------|-----------------------|-|----------------------------------------------------------------|
| linstor.csi.linbit.com/placementCount: "1" | replication: "None"   | | Будет создаваться одна реплика тома с данными                  |
| linstor.csi.linbit.com/placementCount: "2" | replication: "Availability" | | Будет создаваться две реплики тома с данными.                  |
| linstor.csi.linbit.com/placementCount: "3" | replication: "ConsistencyAndAvailability" | да | Будет создаваться три реплики тома с данными                   |
| linstor.csi.linbit.com/storagePool: "name" | storagePool: "name"   | | Название используемого storage pool для хранения               |
| linstor.csi.linbit.com/allowRemoteVolumeAccess: "false" | volumeAccess: "Local" | | Запрещен удаленный доступ Pod к томам с данными (только локальный доступ к диску в пределах Node) |

Кроме них, можно использовать параметры:

- reclaimPolicy (Delete, Retain) - соответствует параметру reclaimPolicy у старого StorageClass
- zones - перечисление зон, которые нужно использовать для размещения ресурсов (прямое указание названия зон в облаке). Обратите внимание, что удаленный доступ Pod к тому с данными возможен только в пределах одной зоны!
- volumeAccess может принимать значения "Local" (доступ строго в пределах Node), "EventuallyLocal" (реплика данных будет синхронизироваться на Node с запущенным Pod спустя некоторое время после запуска), "PreferablyLocal" (удаленный доступ Pod к тому с данными разрешен, volumeBindingMode: WaitForFirstConsumer), "Any" (удаленный доступ Pod к тому с данными разрешен, volumeBindingMode: Immediate)
- При необходимости использовать volumeBindingMode: Immediate нужно выставлять параметр DRBDStorageClass volumeAccess равным Any
