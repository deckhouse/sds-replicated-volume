diff --git a/pkg/extender/extender.go b/pkg/extender/extender.go
index 73c8c307a..531718a0c 100644
--- a/pkg/extender/extender.go
+++ b/pkg/extender/extender.go
@@ -3,14 +3,18 @@ package extender
 import (
 	"context"
 	"encoding/json"
+	"errors"
 	"fmt"
 	"net/http"
+	"net/http/httputil"
+	"os"
 	"strconv"
 	"strings"
 	"sync"
 	"time"
 
 	"github.com/libopenstorage/stork/drivers/volume"
+	storkcache "github.com/libopenstorage/stork/pkg/cache"
 	storklog "github.com/libopenstorage/stork/pkg/log"
 	restore "github.com/libopenstorage/stork/pkg/snapshot/controllers"
 	"github.com/portworx/sched-ops/k8s/core"
@@ -21,6 +25,7 @@ import (
 	"k8s.io/apimachinery/pkg/runtime"
 	"k8s.io/client-go/tools/record"
 	schedulerapi "k8s.io/kube-scheduler/extender/v1"
+	"sigs.k8s.io/controller-runtime/pkg/manager"
 )
 
 const (
@@ -77,9 +82,15 @@ var (
 type Extender struct {
 	Recorder record.EventRecorder
 	Driver   volume.Driver
-	server   *http.Server
-	lock     sync.Mutex
-	started  bool
+	// block added by flant.com
+	CertFile string    `json:"cert-file"`
+	KeyFile  string    `json:"key-file"`
+	LogLevel log.Level `json:"log-level" default:"DebugLevel"`
+	Manager  *manager.Manager
+	// end of flant.com block
+	server  *http.Server
+	lock    sync.Mutex
+	started bool
 }
 
 // Start Starts the extender
@@ -90,13 +101,53 @@ func (e *Extender) Start() error {
 	if e.started {
 		return fmt.Errorf("Extender has already been started")
 	}
+	// block added by flant.com
+	log.SetLevel(e.LogLevel)
+
+	if e.Manager != nil {
+		// Setup stork cache. We setup this cache for all the stork pods instead of just the leader pod.
+		// In this way, even the stork extender code can use this cache, since the extender filter/process
+		// requests can land on any stork pod.
+		if err := storkcache.CreateSharedInformerCache(*e.Manager); err != nil {
+			log.Fatalf("failed to setup shared informer cache: %v", err)
+		}
+		log.Infof("shared informer cache has been intialized")
+	}
+	// end of flant.com block
+
 	// TODO: Make the listen port configurable
 	e.server = &http.Server{Addr: ":8099"}
 	http.HandleFunc("/", e.serveHTTP)
 	go func() {
-		if err := e.server.ListenAndServe(); err != http.ErrServerClosed {
+		// block added by flant.com
+		tlsMode := len(e.CertFile) > 0 && len(e.KeyFile) > 0
+
+		var err error
+
+		if tlsMode {
+			if _, err := os.Stat(e.CertFile); errors.Is(err, os.ErrNotExist) {
+				log.Warnf("CertFile `%s` does not exists. Launch as HTTP", e.CertFile)
+				tlsMode = false
+			}
+
+			if _, err := os.Stat(e.KeyFile); errors.Is(err, os.ErrNotExist) {
+				log.Warnf("KeyFile `%s` does not exists. Launch as HTTP", e.KeyFile)
+				tlsMode = false
+			}
+		}
+
+		if tlsMode {
+			log.Warn("Starting as HTTPS on :8099")
+			err = e.server.ListenAndServeTLS(e.CertFile, e.KeyFile)
+		} else {
+			log.Warn("Starting as HTTP on :8099")
+			err = e.server.ListenAndServe()
+		}
+
+		if err != http.ErrServerClosed {
 			log.Panicf("Error starting extender server: %v", err)
 		}
+		// end of flant.com block
 	}()
 
 	prometheus.MustRegister(HyperConvergedPodsCounter)
@@ -149,6 +200,16 @@ func (e *Extender) getHostname(node *v1.Node) string {
 }
 
 func (e *Extender) processFilterRequest(w http.ResponseWriter, req *http.Request) {
+	// block added by flant.com
+	payload, err := httputil.DumpRequest(req, true)
+	if err != nil {
+		log.Errorf("[filter] Error debugging request: %v", err)
+		http.Error(w, err.Error(), http.StatusInternalServerError)
+		return
+	}
+	log.Tracef("[filter] received request: %s", string(payload))
+	// end of flant.com block
+
 	decoder := json.NewDecoder(req.Body)
 	defer func() {
 		if err := req.Body.Close(); err != nil {
@@ -164,6 +225,14 @@ func (e *Extender) processFilterRequest(w http.ResponseWriter, req *http.Request
 		return
 	}
 
+	// block added by flant.com
+	if err = fillNodesByNames(&args); err != nil {
+		log.Errorf("[filter] Cannot fill nodes: %s", err.Error())
+		http.Error(w, "[filter] Cannot fill nodes", http.StatusBadRequest)
+		return
+	}
+	// end of flant.com block
+
 	pod := args.Pod
 	if pod == nil {
 		msg := "Empty pod received in filter request"
@@ -176,9 +245,24 @@ func (e *Extender) processFilterRequest(w http.ResponseWriter, req *http.Request
 		if vol.PersistentVolumeClaim == nil {
 			continue
 		}
-		pvc, err := core.Instance().GetPersistentVolumeClaim(vol.PersistentVolumeClaim.ClaimName, pod.Namespace)
+
+		var pvc *v1.PersistentVolumeClaim
+		var err error
+		var msg string
+
+		if storkcache.Instance() != nil {
+			pvc, err = storkcache.Instance().GetPersistentVolumeClaim(vol.PersistentVolumeClaim.ClaimName, pod.Namespace)
+			if err != nil {
+				msg = fmt.Sprintf("Unable to find PVC %s in informer cache, err: %s", vol.Name, err.Error())
+			}
+		} else {
+			pvc, err = core.Instance().GetPersistentVolumeClaim(vol.PersistentVolumeClaim.ClaimName, pod.Namespace)
+			if err != nil {
+				msg = fmt.Sprintf("Unable to find PVC %s, err: %s", vol.Name, err.Error())
+			}
+		}
+
 		if err != nil {
-			msg := fmt.Sprintf("Unable to find PVC %s, err: %v", vol.Name, err)
 			storklog.PodLog(pod).Warnf(msg)
 			e.Recorder.Event(pod, v1.EventTypeWarning, schedulingFailureEventReason, msg)
 			http.Error(w, msg, http.StatusBadRequest)
@@ -192,7 +276,7 @@ func (e *Extender) processFilterRequest(w http.ResponseWriter, req *http.Request
 		}
 	}
 
-	storklog.PodLog(pod).Debugf("Nodes in filter request:")
+	storklog.PodLog(pod).Debugf("[filter] Nodes in filter request")
 	for _, node := range args.Nodes.Items {
 		storklog.PodLog(pod).Debugf("%v %+v", node.Name, node.Status.Addresses)
 	}
@@ -385,9 +469,27 @@ func (e *Extender) collectExtenderMetrics() error {
 		return nil
 	}
 
-	if err := core.Instance().WatchPods("", fn, metav1.ListOptions{}); err != nil {
-		log.Errorf("failed to watch pods due to: %v", err)
-		return err
+	podHandler := func(object interface{}) {
+		pod, ok := object.(*v1.Pod)
+		if !ok {
+			log.Errorf("invalid object type on pod watch from cache: %v", object)
+		} else {
+			fn(pod)
+		}
+	}
+
+	if storkcache.Instance() != nil {
+		log.Debugf("Shared informer cache has been initialized, using it for extender metrics.")
+		err := storkcache.Instance().WatchPods(podHandler)
+		if err != nil {
+			log.Errorf("failed to watch pods with informer cache for health monitoring, err: %v", err)
+		}
+	} else {
+		log.Warnf("Shared informer cache has not been initialized, using watch for extender metrics.")
+		if err := core.Instance().WatchPods("", fn, metav1.ListOptions{}); err != nil {
+			log.Errorf("failed to watch pods for metrics due to: %v", err)
+			return err
+		}
 	}
 	return nil
 }
@@ -461,6 +563,16 @@ type localityInfo struct {
 }
 
 func (e *Extender) processPrioritizeRequest(w http.ResponseWriter, req *http.Request) {
+	// block added by flant.com
+	payload, err := httputil.DumpRequest(req, true)
+	if err != nil {
+		log.Errorf("[prioritize] Error debugging request: %v", err)
+		http.Error(w, err.Error(), http.StatusInternalServerError)
+		return
+	}
+	log.Tracef("[prioritize] received request: %s", string(payload))
+	// end of flant.com block
+
 	decoder := json.NewDecoder(req.Body)
 	defer func() {
 		if err := req.Body.Close(); err != nil {
@@ -476,6 +588,14 @@ func (e *Extender) processPrioritizeRequest(w http.ResponseWriter, req *http.Req
 		return
 	}
 
+	// block added by flant.com
+	if err = fillNodesByNames(&args); err != nil {
+		log.Errorf("[prioritize] Cannot fill nodes: %+v", err)
+		http.Error(w, "[prioritize] Cannot fill nodes", http.StatusBadRequest)
+		return
+	}
+	// end of flant.com block
+
 	pod := args.Pod
 	storklog.PodLog(pod).Debugf("Nodes in prioritize request:")
 	for _, node := range args.Nodes.Items {
@@ -495,7 +615,6 @@ func (e *Extender) processPrioritizeRequest(w http.ResponseWriter, req *http.Req
 
 	// Score all nodes the same if hyperconvergence is disabled
 	disableHyperconvergence := false
-	var err error
 	if pod.Annotations != nil {
 		if value, ok := pod.Annotations[disableHyperconvergenceAnnotation]; ok {
 			if disableHyperconvergence, err = strconv.ParseBool(value); err != nil {
@@ -632,3 +751,48 @@ sendResponse:
 		storklog.PodLog(pod).Errorf("Failed to encode response: %v", err)
 	}
 }
+
+// block added by flant.com
+func fillNodesByNames(inputData *schedulerapi.ExtenderArgs) error {
+	if inputData.Nodes != nil && len(inputData.Nodes.Items) > 0 {
+		// nodes already presents in inputData, just return 'as is'
+		return nil
+	}
+
+	if inputData.NodeNames == nil || len(*inputData.NodeNames) == 0 {
+		return fmt.Errorf("no NodeNames")
+	}
+
+	var node *v1.Node
+	var err error
+	var msg string
+
+	nodeList := &v1.NodeList{}
+
+	for _, nodeName := range *inputData.NodeNames {
+		if storkcache.Instance() != nil {
+			node, err = storkcache.Instance().GetNode(nodeName)
+			if err == nil {
+				log.Tracef("Get node %s from cache", nodeName)
+			}
+			msg = fmt.Sprintf("Unable to get node info for node %s from informer cache, err: %v", nodeName, err)
+		} else {
+			node, err = core.Instance().GetNodeByName(nodeName)
+			if err == nil {
+				log.Tracef("Get node %s from API-server", nodeName)
+			}
+			msg = fmt.Sprintf("Unable to get node info for node %s, err: %v", nodeName, err)
+		}
+
+		if err != nil {
+			return fmt.Errorf(msg)
+		}
+
+		nodeList.Items = append(nodeList.Items, *node)
+	}
+
+	inputData.Nodes = nodeList
+	return nil
+}
+
+// end of flant.com block
